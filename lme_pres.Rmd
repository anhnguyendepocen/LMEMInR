---
title: "Linear Mixed-Effect Modeling in R"
author: "Clay Ford"
date: "Fall 2015"
output: beamer_presentation
---

## Workshop Goals

To teach you...

- what linear mixed-effect models are
- when to use linear mixed-effect models
- how to implement in R
- how to interpret the output

## What is a Linear Mixed-Effect Model?

A linear model with both *fixed* and *random* effects. Hence, the phrase "*Mixed-Effect*".

**Fixed effects** are population parameters we seek to estimate. These are regression coefficients for *repeatable* variables (eg: Gender, Age, Treatment, Race, etc.)

**Random effects** are random values associated with levels of a random factor. We want to understand their variability, not estimate them. These are for *non-repeatable* variables (eg: subjects, teachers, schools, plots of land, etc.)

If we repeat the analysis on new data, the *repeatable* variables would be collected or used again, the *non-repeatable* variables would not.

## Example

We have a new treatment for a disease. We carry out an experiment where we randomly prescribe the treatment or placebo to patients and collect response data every two weeks for two months to gauge efficacy. 

- The Fixed Effect is Treatment. If we were to replicate this study, we would use this variable again.  

- The Random Effect is Patient. If we were to replicate this study, we would not use the same patients again. Most importantly, *the response data is grouped by patient*.  

## Another example

We have a new computer-based math curriculum. We carry out an experiment where we randomly prescribe either the new curriculum or current curriculum to teachers in various schools and measure change in student pre- and post-test scores. 

- The Fixed Effect is Curriculum. If we were to replicate this study, we would use this variable again.  

- The Random Effects are Teachers and Schools. If we were to replicate this study, we would not necessarily use the same teachers and schools again. Most importantly, *the response data is grouped by schools and teachers within schools*.


## Data for Linear Mixed-Effect Models

LMM (**L**inear **M**ixed-Effect **M**odels) are suitable for clustered, longitudinal or repeated-measures data in which the data are grouped by *random factors* and the response (or dependent variable) is continuous. 

In our first example, we had longitudinal data grouped by patient (a random factor). The effect of the patient is a random effect. The effect of the treatment is a fixed effect.

In our second example, we had clustered data grouped by schools and teachers within schools (random factors). The effect of the schools and teachers within schools are random effects. The effect of the curriculum is a fixed effect.

## Data in Long Format
In R, LMM data need to be in *long* format. That is, we have one record per subject per each measure of the dependent variable. 

```{r, echo=FALSE, cache=TRUE, message=FALSE}
library(lme4)
head(sleepstudy)
```

The dependent variable is `Reaction`. Notice we have one record per subject per each measure of `Reaction`. Here, `Subject` is the random effect.

## Specification of a Linear Model

Let's review the specification for a basic linear model:

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \epsilon  $$
$$\epsilon \sim N(0, \sigma)$$ 

This says our dependent variable ($\boldsymbol{Y}$) is equal to our independent variables ($\boldsymbol{X}$) multiplied by coefficients ($\boldsymbol{\beta}$) and added up, plus some random error ($\epsilon$) that comes from a Normal distribution with mean 0 and standard deviation $\sigma$.

A key assumption is the independence of the random errors.

## Simple data for a linear model
```{r, echo=FALSE}
options(digits = 4)
x <- seq(0,3,length.out = 100)
set.seed(1)
y <- 0.5 + 0.8*x + rnorm(100,sd = 0.4)
m1 <- lm(y ~ x)
dat <- data.frame(y, x, fit = fitted(m1))
library(ggplot2)
ggplot(dat, aes(x,y)) + geom_point()

```


## Fitting a linear model in R

```{r}
m1 <- lm(y ~ x)
coef(m1) # model coefficients
summary(m1)$sigma # estimate of error standard deviation
```


## Visualizing a linear model

```{r, echo=FALSE, cache=TRUE, message=FALSE}
library(gridExtra)
p1 <- ggplot(dat, aes(x,y)) + geom_point()

p2 <- ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method="lm",se=F) + 
  geom_segment(aes(x = x, y = y, xend = x, yend = fit)) +
  geom_text(data=NULL, aes(x=0,y=3,label="Y == 0.55 + 0.79*X + N(0,0.36)"), 
            parse=TRUE, hjust=0)
grid.arrange(p1, p2, nrow=1) # can also use ncol

```

## Specification of a Linear Mixed-Effect Model

The linear mixed-effect model is more complex:

$$\boldsymbol{Y_i} = \boldsymbol{X_i\beta} + \boldsymbol{Z_ib_i} + \epsilon_i $$
$$\boldsymbol{b_i} \sim N(\boldsymbol{0}, \boldsymbol{D})$$ 
$$\epsilon_i \sim N(\boldsymbol{0}, \boldsymbol{R_i})$$ 

Notice the `i` subscript. Linear mixed-effect models are fit to groups of data, say repeated measures made on subjects. The $\boldsymbol{X_i\beta}$ are the fixed effects. The $\boldsymbol{Z_ib_i}$ are the random effects. The $\epsilon_i$ are the random errors.

In contrast to a standard linear models, random errors can be correlated.

## Random effects and errors

In a standard linear model we just have to estimate one random error parameter: $\sigma$   

In a linear mixed-effect model we have two *matrices* of error structures:   
- one for the random effects: $\boldsymbol{D}$   
- one for the random errors: $\boldsymbol{R_i}$

These matrices can take on different *structures*. 

## What do we mean by "structures"?

The $\boldsymbol{D}$ and $\boldsymbol{R_i}$ matrices are known as *variance-covariance* matrices. 

The variance is on the diagonal. The covariance is on the off-diagonal. For example, a $\boldsymbol{D}$  matrix for two random effects might be structured as follows:

$$\begin{pmatrix}
\sigma_{b1}^{2}  &  \sigma_{b1,b2}\\ 
 \sigma_{b1,b2} & \sigma_{b2}^{2}
\end{pmatrix}$$

In this matrix there are three parameters to estimate: $\sigma_{b1}^{2}$, $\sigma_{b2}^{2}$, and $\sigma_{b1,b2}$. This structure assumes the two random effects have covariance, ie, they vary together. *This is the default in R.* 

## Example of another $\boldsymbol{D}$ matrix structure

We could also constrain the covariance to be 0 in a $\boldsymbol{D}$  matrix, like so:

$$\begin{pmatrix}
\sigma_{b1}^{2}  &  0\\ 
0 & \sigma_{b2}^{2}
\end{pmatrix}$$

In this matrix there are only two parameters to estimate, $\sigma_{b1}^{2}$ and $\sigma_{b2}^{2}$, since we assume covariance is 0.

## Example of a $\boldsymbol{R_i}$ matrix structure

The simplest covariance matrix for the random errors is the diagonal structure:

$$\begin{pmatrix}
\sigma^{2}  & 0 & \cdots  & 0\\ 
0 & \sigma^{2} & \cdots & 0\\ 
\vdots  & \vdots & \ddots  &\vdots \\ 
0 & 0 & \cdots & \sigma^{2}
\end{pmatrix}$$

This says that residuals associated with observations on the same subject have equal variance and are uncorrelated. We only have one parameter to estimate. This is the same as the standard linear model. *This is the default in R.*

## Another example of a $\boldsymbol{R_i}$ matrix structure

The following is called a *compound symmetry* structure.

$$\begin{pmatrix}
\sigma^{2} + \sigma_{1}  & \sigma_{1} & \cdots  & \sigma_{1}\\ 
\sigma_{1} & \sigma^{2} + \sigma_{1} & \cdots & \sigma_{1}\\ 
\vdots  & \vdots & \ddots  &\vdots \\ 
\sigma_{1} & \sigma_{1} & \cdots & \sigma^{2} + \sigma_{1}
\end{pmatrix}$$

This says that residuals associated with observations on the same subject have equal variance and equal covariance. This is used when an assumption of equal correlation of residuals is plausible. Here we have two parameters to estimate: $\sigma^{2}$ and $\sigma_{1}$

There are other covariance structures commonly used for $\boldsymbol{R_i}$, but we won't cover those today.


## Simple data for a linear mixed-effect model
```{r, cache=TRUE, echo=FALSE}
n1 <- 10 # 10 subjects
n2 <- 10 # 10 obs per subject
g <- gl(n = n1, k = n2) # 10 levels repeated 10 times each
set.seed(1)
x <- as.vector(replicate(n1,round(sort(runif(n2, 0, 5)),3)))
b0 <- rnorm(n1, 0, 0.8) # random intercept
eps <- rnorm(length(g),0,0.2)
b <- b0[g]
y <- 0.5 + b + 1.5*x + eps
dat <- data.frame(g, x, y, b, eps)
p1 <- ggplot(dat, aes(x,y)) + geom_point() + labs(title="with no grouping indicated")
p2 <- ggplot(dat, aes(x,y, color=g)) + geom_point() + geom_line() + labs(title="with grouping indicated")
grid.arrange(p1, p2, nrow=1)
```


## What do we notice about the data?

- The different groups seem to have the same trajectory, or *slope*
- The different groups seem to have different starting points, or *intercepts*
- Each group of data appears as if it could be modeled by a straight line model

If we were to imagine what kind of mathematical process gave rise to this data, we might propose a straight line model with a *fixed* slope coefficient but a *random* intercept.

## Fitting a linear mixed-effect model in R

Fit a model with one random effect: the intercept

```{r, message=FALSE}
library(lme4) # package for fitting lmm
lme1 <- lmer(y ~ x + (1 | g), data = dat)
fixef(lme1) # fixed effect estimates
VarCorr(lme1) # D and R estimates
```

## Visualizing a linear mixed-effect model

```{r, echo=FALSE, cache=TRUE}
lfits <- coef(lme1)$g
names(lfits)[1] <- "int"
lfits$g <- factor(1:10)
ggplot(dat, aes(x,y,color=g)) + geom_point() + 
  geom_abline(data=lfits, aes(intercept=int, slope=x, color=g)) +
  geom_abline(intercept=fixef(lme1)[1], slope=fixef(lme1)[2],size=2, alpha=0.4) +
  labs(title="with grouping indicated and fitted linear mixed-effect model") +
  annotate("text", x=0,y=8,label="Y == (0.61 + N(0,0.83)[g]) + 0.49*X  + N(0,0.19)", 
            parse=TRUE, hjust=0, size = 5)
```

## Random slopes

```{r,echo=FALSE, cache=TRUE}
# Random slopes
n1 <- 10 # 10 subjects
n2 <- 10 # 10 obs per subject
g <- gl(n = n1, k = n2) # 10 levels repeated 10 times each
set.seed(1)
x <- as.vector(replicate(n1,round(sort(runif(n2, 0, 5)),3)))
b1 <- rnorm(n1, 0, 0.8) # random slope
eps <- rnorm(length(g),0,0.2)
b <- b1[g]
y <- 0.5 + (1.5 + b)*x + eps
dat <- data.frame(g, x, y, b, eps)
p1 <- ggplot(dat, aes(x,y)) + geom_point() + labs(title="with no grouping indicated")
p2 <- ggplot(dat, aes(x,y, color=g)) + geom_point() + geom_line() + labs(title="with grouping indicated")
grid.arrange(p1, p2, nrow=1) 
```

## What do we notice about the data?

- The different groups seem to have the same intercept
- The different groups seem to have *different slopes*
- Each group of data appears as if it could be modeled by a straight line model

If we were to imagine what kind of mathematical process gave rise to this data, we might propose a straight line model with a *fixed* intercept coefficient but a *random* slope coefficient.

## Fitting a LMM for random slopes

```{r, message=FALSE}
# fit random effect for slope but not intercept
lme2 <- lmer(y ~ x + (0 + x | g), data = dat)
fixef(lme2) # fixed effect estimates
VarCorr(lme2) # D and R estimates
```

## Visualizing the fit

```{r, echo=FALSE, cache=TRUE}
lfits <- coef(lme2)$g
names(lfits)[1] <- "int"
lfits$g <- factor(1:10)
ggplot(dat, aes(x,y,color=g)) + geom_point() + 
  geom_abline(data=lfits, aes(intercept=int, slope=x, color=g)) +
  geom_abline(intercept=fixef(lme2)[1], slope=fixef(lme2)[2],size=2, alpha=0.4) +
  labs(title="with grouping indicated and fitted linear mixed-effect model") +
  annotate("text", x=0,y=11,label="Y == 0.49 + (1.61 + N(0,0.82)[g])*X  + N(0,0.19)", 
            parse=TRUE, hjust=0, size = 5)
```


## Random intercepts and slopes

```{r,echo=FALSE, cache=TRUE}
# Random intercepts and random slopes
n1 <- 10 # 10 subjects
n2 <- 10 # 10 obs per subject
g <- gl(n = n1, k = n2) # 10 levels repeated 10 times each
set.seed(1)
x <- as.vector(replicate(n1,round(sort(runif(n2, 0, 5)),3)))
b0 <- rnorm(n1, 0, 1) # random intercept
b1 <- rnorm(n1, 0, 0.8) # random slope
eps <- rnorm(length(g),0,0.2)
b0 <- b0[g]
b1 <- b1[g]
y <- (0.5 + b0) + (1.5 + b1)*x + eps
dat <- data.frame(g, x, y, eps)
p1 <- ggplot(dat, aes(x,y)) + geom_point() + labs(title="with no grouping indicated")
p2 <- ggplot(dat, aes(x,y, color=g)) + geom_point() + geom_line() + labs(title="with grouping indicated")
grid.arrange(p1, p2, nrow=1)
```

## Fitting a LMM for random intercepts and slopes

```{r, message=FALSE}
# fit random effects for intercepts and slopes 
lme3 <- lmer(y ~ x + (x | g), data = dat)
fixef(lme3) # fixed effect estimates
VarCorr(lme3) # D and R estimates
```

## Visualizing the fit

```{r, echo=FALSE, cache=TRUE}
lfits <- coef(lme3)$g
names(lfits)[1] <- "int"
lfits$g <- factor(1:10)
ggplot(dat, aes(x,y,color=g)) + geom_point() + 
  geom_abline(data=lfits, aes(intercept=int, slope=x, color=g)) +
  geom_abline(intercept=fixef(lme3)[1], slope=fixef(lme3)[2],size=2, alpha=0.4) +
  labs(title="with grouping indicated and fitted linear mixed-effect model") +
  annotate("text", x=0,y=13,label="Y == (0.61 + N(0,1.07)[g]) + (1.87 + N(0,1.02)[g])*X  + N(0,0.19)", 
            parse=TRUE, hjust=0, size = 5)
```


## Packages for fitting LMM

The two most commonly used R packages for fitting linear mixed-effect models are `nlme` and `lme4`.

- **`nlme`**: older package, comes with R, very stable; can fit linear and non-linear mixed-effect models; allows fitting of various covariance structures for random effects and residual errors.

- **`lme4`**: newer package, does not come with R; can fit linear, generalized linear, and nonlinear mixed-effect models; can also fit models with *crossed random effects*; does not currently allow fitting various covariance structures for residual errors.

`lme4` also uses different computations than `nlme`. The math is far beyond the scope of this workshop but suffice to say it makes lme4 better for larger data.

## `lme4`

For today's workshop we'll use `lme4`. 

The syntax is a little easier to learn and it seems to be the go-to package in the R community for linear mixed-effect modeling. 

It's worth repeating: `lme4` does not currently provide facilities for modeling different variance-covariance structures for residuals, $\boldsymbol{R_i}$. Residual errors are assumed to be Normally distributed with mean 0 and variance $\sigma^2$. 

By the way, the "4" in `lme4` refers to the technical fact that `lme4` was programmed using the S4 system in R.

## Fitting a model with `lme4`

Assuming one level of grouping, say repeated measures on subjects.

**Random intercept**:  
`lme1 <- lmer(dv ~ x1 + (1 | g), data=df)`

**Random slope**:  
`lme2 <- lmer(dv ~ x1 + (0 + x1 | g), data=df)`

**Correlated random slope and intercept**:  
`lme3 <- lmer(dv ~ x1 + (x1 | g), data=df)`

**Uncorrelated random slope and intercept**:   
`lme3 <- lmer(dv ~ x1 + (x1 || g), data=df)`

## Fitting a model with `lme4`

Assuming two levels of grouping, say students clustered in teachers, and teachers within schools:

**Random intercept**:  
`lme1 <- lmer(dv ~ x1 + (1 | sch/tch), data=df)`

**Random slope**:  
`lme2 <- lmer(dv ~ x1 + (0 + x1 | sch/tch), data=df)`

**Correlated random slope and intercept**:  
`lme3 <- lmer(dv ~ x1 + (x1 | sch/tch), data=df)`

**Uncorrelated random slope and intercept**:   
`lme3 <- lmer(dv ~ x1 + (x1 || sch/tch), data=df)`

## Selected `lme4` extractor functions

Once we fit a model, `lme4` has several functions for extracting and viewing model information:

- `summary()` - View summary of fitted LMM 
- `fixef()` - View estimated fixed effect coefficients 
- `ranef()` - View predicted random effects
- `coef()` - View coefficients for LMM for *each group* 
- `VarCorr()` - View estimated variance parameters

Let's go to R!

## Why no p-values in the LMM summary?

Recall that p-values in the coefficient summary of fitted model are the probability of getting a test statistic as large (or larger) if the coefficient was indeed 0.

Also recall that p-values are determined using null reference distributions. Under the null hypothesis, the test statistic has a known distribution.

In LMM, the null reference distribution is technically not known, at least not for unbalanced data. Thus the `lme4` author elected to not output p-values based on a distribution that is not actually the distribution of the test statistic.

## How do I know if a coefficient is "significant"?

One way is to compute 95% confidence intervals using the `confint()` function. If the interval contains 0, it is not significant. Further, confidence intervals give an indication of coefficient size and variability.

Say your fitted model object is `lme1`. Two ways to compute confidence intervals are as follows:

`confint(lme1)`

> computes a likelihood profile and finds the appropriate cutoffs based on the likelihood ratio test

`confint(lme1, method="boot")`

> parametric bootstrapping (B = 500) with confidence intervals computed from the bootstrap distribution

## But I want p-values!

The `lmerTest` package provides the kind of p-values SAS provides (based on Satterthwaite's approximations). Just load it, run `lmer` as usual and call `summary` on the object.

`library(lmerTest)`   
`m <- lmer(dv ~ x1 + (x1 | g), data=df)`   
`summary(m)`   

A column of p-values is included in the summary output. Again, they're *approximate*.

## Assessing significance of fixed-effect factors

Fixed-effect factors with more than two levels should be assessed with `anova`, not by examining p-values in the `summary` output (which is easy since there are none in `lme4` summaries).

Say your fitted model object `lme1` contains fixed-effect factors. To assess their significance:

`anova(lme1)`

Two things to note:

1. The tests are *sequential*. (aka, Type I)
2. No p-values are provided. Same explanation as before.

## But I want p-values!

The `car` package provides a functions called `Anova` that provides approximate p-values and tests each predictor after all others (Type II) as opposed to sequentially (Type I). 

To use with, say, a fitted model called `lme1`:

`library(car)`   
`Anova(lme1)`


## Diagnostic plots

Once we fit a LMM, we should assess our assumptions:

1. within-group errors (ie, residuals) are normally distributed, centered at 0 and have constant variance

2. random effects are normally distributed, centered at 0 and have constant variance

`lme4` provides a `plot()` method to help graphically check the variance assumptions of residuals. Say you have a fitted model object named `lme1`. The basic syntax is as follows:

`plot(lme1, form=)`

Where the `form=` argument is an optional formula specifying the desired type of plot.

## Examples of the `form=` argument

Say we fit the folllowing LMM:   
`lme1 <- lmer(y ~ x + (x | id), data=df)`

**standarized residuals vs fitted values**:   
`plot(lme1)`

**standardized residuals versus fitted values by x**:    
`plot(lme1, form = resid(.) ~ fitted(.) | x)`

**box-plots of residuals by id**:    
`plot(lme1, form = id ~ resid(.))`

**observed versus fitted values by id**:   
`plot(lme1, y ~ fitted(.) | id)`


## Diagnostic plots continued

**To check constant variance of random effects**:   
`plot(ranef(lme1))`

**To assess normality of residuals**:   
`qqnorm(resid(lme1))`  

**To asses normality of random effects**:     
`library(lattice)`   
`qqmath(ranef(lme1))`

Let's go to R.

## REML vs. ML

If you look closely at the summary output for a model fit with lme4, you'll see the message: `Linear mixed model fit by REML`.

REML stands for Restricted Maximum-Likelihood Estimation. It provides unbiased estimates for the variance parameters. 

By setting `REML=FALSE` in the `lmer()` function, we can estimate parameters using ML, Maximum Likelihood. But the variance estimates will be biased downwards.

The choice of ML vs REML affects model comparisons.


## Model Comparisons

We often compare models to see if a smaller model fits as well as a larger model.

We can compare models either by hypothesis testing or by selection criteria (such as AIC or BIC).

When it comes to hypothesis testing, the choice of ML vs REML estimation is important:

> To compare two models fit by REML, each must have the same fixed effects. 

> To compare two models fit by ML, one must be nested within the other. 

The function to compare models is `anova()`. Its default behavior is to refit the models with ML before comparing. Specify `refit=FALSE` to supress refitting.

## Comparing nested models

Say we fit two models:   
`lme1 <- lmer(y ~ x + z + x:z + (1 | g), data=df)`   
`lme2 <- lmer(y ~ x + z + (1 | g), data=df)`   

The following refits the models with ML and compares them via hypothesis test:   
`anova(lme1, lme2)`

Notice that `lme2` is nested in `lme1`. That is, `lme2` is a special case of `lme1` with an interaction coefficient for `x:z` equal to 0.

The null hypothesis states the models are equal. Rejecting the null means we prefer the larger model.

## Comparing models with different random effects

Random effect parameters are measures of variances. Variance is greater than or equal to 0. 

When testing whether or not to include a random effect in a model, we're testing that its variance is 0. In that case our hypothesis test involves values lying on the boundary of the parameter space.

The result is that the standard hypothesis test (ie, a likelihood ratio test) is *conservative*. The p-value is too high.

If you have a small p-value (say < 0.001), that's not a problem. 

If you have a p-value close to significance, (say about 0.10) you may want to consider calculating a corrected p-value using a *mixture of chi-square distributions*.

## A mixture of chi-square distributions

A likelihood ratio test (LRT) statistic has a chi-square distribution with degrees of freedom equal to the difference in parameters between two models.

Let's say our LRT is on 2 degrees of freedom. The null distribution of this test statistic is NOT a chi-square with 2 degrees of freedom since our null value (variance = 0) is on the boundary of the parameter space.

It's been suggested that a 50:50 mixture, $0.5\chi_{df}^{2} + 0.5\chi_{df-1}^{2}$, can serve as a reference null distribution for computing the p-value. But this is still only an approximation.

We'll demonstrate this in the R script. Let's go!



## References

Faraway, J. (2006). *Extending the Linear Model with R*. Chapman and Hall/CRC.

Fox, J. & Weisberg, S. (2015). *Mixed-Effects Models in R: an appendix to An R Companion to Applied Regression*.

Galecki, A. and Burzykowski T. (2013). *Linear Mixed-Effect Models Using R*. Springer.

Pinheiro, J. & Bates, D. (2000). *Mixed-Effects Models in S and S-PLUS*. Springer.

West, B., Welch, K., & Galecki, A. (2015) *Linear Mixed Models*. Chapman and Hall/CRC.

DRAFT r-sig-mixed-models FAQ:   
http://glmm.wikidot.com/faq


## StatLab

Thanks for coming today!

For help and advice with your data analysis, contact the StatLab to set up an appointment: statlab@virginia.edu

Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/statlab/

Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/

