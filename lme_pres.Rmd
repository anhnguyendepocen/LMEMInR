---
title: "Linear Mixed-Effect Modeling in R"
author: "Clay Ford"
date: "Fall 2015"
output: beamer_presentation
---

## Workshop Goals

To teach you...

- what linear mixed-effect models are
- when to use linear mixed-effect models
- how to implement in R
- how to interpret the output

## What is a Linear Mixed-Effect Model?

A linear model with both *fixed* and *random* effects. Hence, the phrase "*Mixed-Effect*".

**Fixed effects** are population parameters we seek to estimate. These are regression coefficients for *repeatable* variables (eg: Gender, Age, Treatment, Race, etc.)

**Random effects** are random values associated with levels of a random factor. We want to understand their variability, not estimate them. These are for *non-repeatable* variables (eg: subjects, teachers, schools, plots of land, etc.)

If we repeat the analysis on new data, the *repeatable* variables would be collected or used again, the *non-repeatable* variables would not.

## Example

We have a new treatment for a disease. We carry out an experiment and randomly prescribe the treatment or placebo to patients and collect response data every two weeks for two months to gauge efficacy. 

- The Fixed Effect is Treatment. If we were to replicate this study, we would use this variable again.  

- The Random Effect is Patient. If we were to replicate this study, we would not use the same patients again. The response data is grouped by patient.  

## Another example

We have a new computer-based math curriculum. We carry out an experiment and randomly prescribe either the new curriculum or current curriculum to teachers within a school district and measure change in student pre- and post-test scores. 

- The Fixed Effect is Curriculum. If we were to replicate this study, we would use this variable again.  

- The Random Effect is Teacher. If we were to replicate this study, we would not use the same teachers again. The response data is grouped by teacher.  


## Data for Linear Mixed-Effect Models

LMM (**L**inear **M**ixed-Effect **M**odels) are suitable for clustered, longitudinal or repeated-measures data in which the data are grouped by *random factors* and the response (or dependent variable) is continuous. 

In our first example, we had longitudinal data grouped by patient (a random factor). The effect of the patient is a random effect. The effect of the treatment is a fixed effect.

In our second example, we had clustered data grouped by teacher (a random factor). The effect of the teacher is a random effect. The effect of the curriculum is a fixed effect.

## Data in Long Format
In R, LMM data need to be in *long* format. That is, we have one record per subject per each measure of the dependent variable. 

```{r, echo=FALSE, cache=TRUE, message=FALSE}
library(lme4)
head(sleepstudy)
```

The dependent variable is `Reaction`. Notice we have one record per subject per each measure of `Reaction`.

## Specification of a Linear Model

Let's review the specification for a basic linear model:

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \epsilon  $$
$$\epsilon \sim N(0, \sigma)$$ 

This says our dependent variable ($\boldsymbol{Y}$) is equal to our independent variables ($\boldsymbol{X}$) multiplied by coefficients ($\boldsymbol{\beta}$) and added up, plus some random error ($\epsilon$) that comes from a Normal distribution with mean 0 and standard deviation $\sigma$.

A key assumption is the independence of the random errors.

## Simple data for a linear model
```{r, echo=FALSE}
options(digits = 4)
x <- seq(0,3,length.out = 100)
set.seed(1)
y <- 0.5 + 0.8*x + rnorm(100,sd = 0.4)
m1 <- lm(y ~ x)
dat <- data.frame(y, x, fit = fitted(m1))
library(ggplot2)
ggplot(dat, aes(x,y)) + geom_point()

```


## Fitting a linear model in R

```{r}
m1 <- lm(y ~ x)
coef(m1) # estimate coefficients
summary(m1)$sigma # estimate of error standard deviation
```


## Visualizing a linear model

```{r, echo=FALSE, cache=TRUE, message=FALSE}
library(gridExtra)
p1 <- ggplot(dat, aes(x,y)) + geom_point()

p2 <- ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method="lm",se=F) + 
  geom_segment(aes(x = x, y = y, xend = x, yend = fit)) +
  geom_text(data=NULL, aes(x=0,y=3,label="Y == 0.55 + 0.79*X + N(0,0.36)"), 
            parse=TRUE, hjust=0)
grid.arrange(p1, p2, nrow=1) # can also use ncol

```

## Specification of a Linear Mixed-Effect Model

The linear mixed-effect model is more complex:

$$\boldsymbol{Y_i} = \boldsymbol{X_i\beta} + \boldsymbol{Z_ib_i} + \epsilon_i $$
$$\boldsymbol{b_i} \sim N(\boldsymbol{0}, \boldsymbol{D})$$ 
$$\epsilon_i \sim N(\boldsymbol{0}, \boldsymbol{R_i})$$ 

Notice the `i` subscript. Linear mixed-effect models are fit to groups of data, say repeated measures made on subjects. The $\boldsymbol{X_i\beta}$ are the fixed effects. The $\boldsymbol{Z_ib_i}$ are the random effects. The $\epsilon_i$ are the random errors.

In contrast to a standard linear models, random errors can be correlated.

## Random effects and errors

In a standard linear model we just have to estimate one random error parameter: $\sigma$   

In a linear mixed-effect model we have two *matrices* of error structures:   
- one for the random effects: $\boldsymbol{D}$   
- one for the random errors: $\boldsymbol{R_i}$

These matrices can take on different *structures*. 

## What do we mean by "structures"?

The $\boldsymbol{D}$ and $\boldsymbol{R_i}$ matrices are known as *variance-covariance* matrices. 

The variance is on the diagonal. The covariance is on the off-diagonal. For example, a $\boldsymbol{D}$  matrix for two random effects might be structured as follows:

$$\begin{pmatrix}
\sigma_{b1}^{2}  &  \sigma_{b1,b2}\\ 
 \sigma_{b1,b2} & \sigma_{b2}^{2}
\end{pmatrix}$$

In this matrix there are three parameters to estimate: $\sigma_{b1}^{2}$, $\sigma_{b2}^{2}$, and $\sigma_{b1,b2}$. This structure assumes the two random effects have covariance, ie, they vary together. *This is the default in R.* 

## Example of another $\boldsymbol{D}$ matrix structure

We could also constrain the covariance to be 0 in a $\boldsymbol{D}$  matrix, like so:

$$\begin{pmatrix}
\sigma_{b1}^{2}  &  0\\ 
0 & \sigma_{b2}^{2}
\end{pmatrix}$$

In this matrix there are only two parameters to estimate, $\sigma_{b1}^{2}$ and $\sigma_{b2}^{2}$, since we assume covariance is 0.

## Example of a $\boldsymbol{R_i}$ matrix structure

The simplest covariance matrix for the random errors is the diagonal structure:

$$\begin{pmatrix}
\sigma^{2}  & 0 & \cdots  & 0\\ 
0 & \sigma^{2} & \cdots & 0\\ 
\vdots  & \vdots & \ddots  &\vdots \\ 
0 & 0 & \cdots & \sigma^{2}
\end{pmatrix}$$

This says that residuals associated with observations on the same subject have equal variance and are uncorrelated. We only have one parameter to estimate. This is the same as the standard linear model. *This is the default in R.*

## Another example of a $\boldsymbol{R_i}$ matrix structure

The following is called a *compound symmetry* structure.

$$\begin{pmatrix}
\sigma^{2} + \sigma_{1}  & \sigma_{1} & \cdots  & \sigma_{1}\\ 
\sigma_{1} & \sigma^{2} + \sigma_{1} & \cdots & \sigma_{1}\\ 
\vdots  & \vdots & \ddots  &\vdots \\ 
\sigma_{1} & \sigma_{1} & \cdots & \sigma^{2} + \sigma_{1}
\end{pmatrix}$$

This says that residuals associated with observations on the same subject have equal variance and equal covariance. This is used when an assumption of equal correlation of residuals is plausible. Here we have two parameters to estimate: $\sigma^{2} + \sigma_{1}$

There are other covariance structures commonly used for $\boldsymbol{R_i}$, but we won't cover those today.

## Simple data for a linear mixed-effect model
```{r, cache=TRUE, echo=FALSE}
n1 <- 10 # 10 subjects
n2 <- 10 # 10 obs per subject
g <- gl(n = n1, k = n2) # 10 levels repeated 10 times each
set.seed(1)
x <- as.vector(replicate(n1,round(sort(runif(n2, 0, 5)),3)))
b0 <- rnorm(n1, 0, 0.8) # random intercept
eps <- rnorm(length(g),0,0.2)
b <- b0[g]
y <- 0.5 + b + 1.5*x + eps
dat <- data.frame(g, x, y, b, eps)
p1 <- ggplot(dat, aes(x,y)) + geom_point() + labs(title="with no grouping indicated")
p2 <- ggplot(dat, aes(x,y, color=g)) + geom_point() + geom_line() + labs(title="with grouping indicated")
grid.arrange(p1, p2, nrow=1) # can also use ncol
```


## Fitting a linear mixed-effect model in R

Fit a model with one random effect: the intercept

```{r, message=FALSE}
library(lme4) # package for fitting lmm
lme1 <- lmer(y ~ x + (1 | g), data = dat)
fixef(lme1) # fixed effect estimates
VarCorr(lme1) # D and R estimates
```

## Visualizing a linear mixed-effect model

```{r, echo=FALSE, cache=TRUE}
p1 <- ggplot(dat, aes(x,y, color=g)) + geom_point() + geom_line() + labs(title="with grouping indicated")
lfits <- coef(lme1)$g
names(lfits)[1] <- "int"
lfits$g <- factor(1:10)
p2 <- ggplot(dat, aes(x,y,color=g)) + geom_point() + 
  geom_abline(data=lfits, aes(intercept=int, slope=x, color=g)) +
  geom_abline(intercept=fixef(lme1)[1], slope=fixef(lme1)[2],size=2, alpha=0.4) +
  labs(title="with grouping indicated and fitted linear mixed-effect model") +
  annotate("text", x=0,y=8,label="Y == 0.61 + 0.49*X + N(0,0.83)[g] + N(0,0.19)", 
            parse=TRUE, hjust=0, size = 3)
grid.arrange(p1, p2, nrow=1) 
```

## Random intercepts and random slopes


## Packages for fitting LMM

The two most commonly used R packages for fitting linear mixed-effect models are `nlme` and `lme4`.

- **`nlme`**: older package, comes with R, very stable; can fit linear and non-linear mixed-effect models; allows fitting of various covariance structures for random effects and residual errors.

- **`lme4`**: newer package, does not come with R; can fit linear, generalized linear, and nonlinear mixed-effect models; can also fit models with *crossed random effects*; does not currently allow fitting various covariance structures for residual errors.

`lme4` also use different computations than `nlme`. The math is far beyond the scope of this workshop but suffice to say it makes lme4 better for larger data.

## lme4

For today's workshop we'll use `lme4`. 

The syntax is a little easier to learn and it seems to be the go-to package in the R community for linear mixed-effect modeling. 

By the way, the "4" in `lme4` refers to the technical fact that `lme4` was programmed using the S4 system in R.

Let's go to R!

## References

Linear Mixed Models.

Linear Mixed-Effect Models Using R

Mixed-Effects Models in R

Extending the Linear Model with R. Faraway

## StatLab

Thanks for coming today!

For help and advice with your data analysis, contact the StatLab to set up an appointment: statlab@virginia.edu

Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/statlab/

Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/

